#!/usr/bin/env python

import itertools
import sys
from argparse import ArgumentParser
from collections.abc import Iterable, Iterator, Sequence
from enum import Enum
from pathlib import Path
from typing import NamedTuple, TypeAlias, TypeVar

from clang import cindex  # type: ignore
from clang.cindex import CursorKind  # type: ignore

_T = TypeVar("_T")


_DEPTOOL_BANNER: str = "AUTOGENERATED BY ABC-MINI DEPTOOL... DO NOT MODIFY"
_DEPTOOL_FOOTER: str = "END AUTOGENERATED ABC-MINI DEPTOOL"


def make_comment(line: str) -> str:
    return "// " + line


def flatten(nest: Iterable[Iterable[_T]]) -> Iterator[_T]:
    yield from itertools.chain.from_iterable(nest)


def eprint(*values: object) -> None:
    print(*values, file=sys.stderr)


Trivia: TypeAlias = Sequence[str | None]


class DepTokenKind(Enum):
    BASED_ON = 0
    ALIAS_OF = 1
    DEFINED_IN = 2


class DepToken(NamedTuple):
    kind: DepTokenKind
    trivia: list[str | None]


def tokenize_based_on(
    lines: Sequence[str],
    start: int,
    base: str,
) -> tuple[int, DepToken]:
    trivia = flatten(entry.split(",") for entry in base.split(";"))
    i = start
    return i, DepToken(kind=DepTokenKind.BASED_ON, trivia=list(trivia))


def tokenize_alias_of(
    lines: Sequence[str],
    start: int,
    alias: str,
) -> tuple[int, DepToken]:
    i = start
    line = lines[i].strip()
    tokens = (token.strip(";") for token in line.split()[0:2])
    trivia = [alias, None, *tokens]
    return i, DepToken(kind=DepTokenKind.ALIAS_OF, trivia=trivia)


def tokenize_defined_in(
    lines: Sequence[str],
    start: int,
    defined_in: str,
) -> tuple[int, DepToken]:
    trivia: list[str | None] = [defined_in]
    i = start
    function_signatures: str = ""
    while i < len(lines):
        line = lines[i].strip()
        if line != "// ABC_MINI: Defined-in-end":
            i += 1
        else:
            break
        function_signatures += line + "\n"
    idx = cindex.Index.create()
    unit = idx.parse("funcs.h", unsaved_files=[("funcs.h", function_signatures)])
    for node in unit.cursor.get_children():
        breakpoint()
        if node.kind != CursorKind.FUNCTION_DECL:
            continue
        for part in node.get_children():
            if part.kind != CursorKind.PARM_DECL:
                continue
    return i, DepToken(kind=DepTokenKind.DEFINED_IN, trivia=trivia)


def tokenize(sourcefile: Path) -> Iterator[tuple[int, DepToken]]:
    lines = sourcefile.read_text().splitlines()
    i = 0
    while i < len(lines):
        line = lines[i].strip()
        if not line.startswith("// ABC_MINI:"):
            i += 1
            continue
        source_location = f"{sourcefile.name}:{i + 1}"
        jump = line.find(":") + 1
        line = line[jump:].strip()
        params = [p.strip() for p in line.split(":")]
        if len(params) == 0:
            eprint(f'warning: ignoring empty entry at "{source_location}"')
            i += 1
            continue
        start = i
        match params:
            case ["Based-on", base]:
                i, token = tokenize_based_on(lines, i, base)
            case ["Alias-of", alias]:
                i, token = tokenize_alias_of(lines, i + 1, alias)
            case ["Defined-in-start", defined_in]:
                i, token = tokenize_defined_in(lines, i + 1, defined_in)
            case ["Defined-in-end"]:
                eprint(
                    f'warning: unmatched "Defined-in-end" found at "{source_location}"',
                )
                i += 1
                continue
            case [directive] | [directive, _]:
                eprint(
                    f'warning: unknown directive "{directive}" found at "{source_location}"',
                )
                i += 1
                continue
        yield start, token
        i += 1


def find_tokens(searchdir: Path) -> Iterator[tuple[Path, int, DepToken]]:
    for dirpath, _, filenames in searchdir.walk():
        sources = (f for file in filenames if (f := dirpath / file).suffix == ".cpp")
        for source in sources:
            yield from ((source, line, token) for line, token in tokenize(source))


class DepToolMode(Enum):
    SCAN = 0


def dump_trivia(trivia: Trivia) -> str:
    return "\n".join("---" if fact is None else f"- {fact}" for fact in trivia)


def handle_based_on(
    sourcefile: Path,
    start_line: int,
    mode: DepToolMode,
    trivia: Trivia,
) -> None:
    if mode != DepToolMode.SCAN:
        return
    print(f"HandleBasedOn: ({sourcefile}:{start_line})")
    print(dump_trivia(trivia))


def handle_alias_of(
    sourcefile: Path,
    start_line: int,
    mode: DepToolMode,
    trivia: Trivia,
) -> None:
    if mode != DepToolMode.SCAN:
        return
    print(f"HandleAliasOf: ({sourcefile}:{start_line})")
    print(dump_trivia(trivia))


def handle_defined_in(
    sourcefile: Path,
    start_line: int,
    mode: DepToolMode,
    trivia: Trivia,
) -> None:
    if mode != DepToolMode.SCAN:
        return
    print(f"HandleDefinedIn: ({sourcefile}:{start_line})")
    print(dump_trivia(trivia))


def main(args: Sequence[str] | None = None) -> int:
    argparser = ArgumentParser()
    argparser.add_argument("searchdir", type=Path, default=Path("."))
    parsed, args = argparser.parse_known_args(args)
    mode = DepToolMode.SCAN
    for sourcefile, line, token in find_tokens(parsed.searchdir):
        match token.kind:
            case DepTokenKind.BASED_ON:
                callback = handle_based_on
            case DepTokenKind.ALIAS_OF:
                callback = handle_alias_of
            case DepTokenKind.DEFINED_IN:
                callback = handle_defined_in
            case _:
                continue
        callback(sourcefile, line + 1, mode, token.trivia)
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
